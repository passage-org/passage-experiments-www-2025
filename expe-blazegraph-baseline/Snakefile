import os
import glob
from pathlib import Path

# Configurable CPU count for Docker
PARENT_DIR = "/GDD/Thi/sage-jena-benchmarks"

cpus = config.get("cpus", 1)

query_dirs = config.get("query_dirs", ["wdbench-multiple-tps", "wdbench-opts"])
run_ids = config.get("run_ids", ["run_1","run_2","run_3"])

# Function to collect queries from each QUERY_DIR
def get_queries(query_dir):
    return [Path(f).stem for f in glob.glob(f"{PARENT_DIR}/selected_queries/{query_dir}/*.sparql")]

rule all:
    input:
        expand(f"{PARENT_DIR}/expe-blazegraph-baseline/{{query_dir}}/report-{{query_dir}}-{{cpus}}-cpus.csv", run_id=run_ids, query_dir=query_dirs, cpus=[cpus])


rule execute_one_query:
    input:
        img=f"{PARENT_DIR}/expe-blazegraph-baseline/docker/bg-cli.tar",
        jar=f"{PARENT_DIR}/expe-blazegraph-baseline/blazegraph-cli.jar",
        data=f"{PARENT_DIR}/datasets/wdbench-blaze/blazegraph.jnl",
        query=lambda wildcards: f"{PARENT_DIR}/selected_queries/{wildcards.query_dir}/{wildcards.query}.sparql"
    output:
        result=temp(f"{PARENT_DIR}/expe-blazegraph-baseline/{{query_dir}}/{{cpus}}-cpus/{{run_id}}/{{query}}.dat"),
        nb_results=f"{PARENT_DIR}/expe-blazegraph-baseline/{{query_dir}}/{{cpus}}-cpus/{{run_id}}/{{query}}.nb.dat",
        time=f"{PARENT_DIR}/expe-blazegraph-baseline/{{query_dir}}/{{cpus}}-cpus/{{run_id}}/{{query}}.time.dat"
    run:
        query_name = Path(input.query).stem
        docker_query_file = f"/{Path(input.query).name}"
        docker_result = f"/{wildcards.query_dir}/{wildcards.cpus}-cpus/{wildcards.run_id}/{query_name}.dat"
        docker_nb_results = f"/{wildcards.query_dir}/{wildcards.cpus}-cpus/{wildcards.run_id}/{query_name}.nb.dat"
        docker_time = f"/{wildcards.query_dir}/{wildcards.cpus}-cpus/{wildcards.run_id}/{query_name}.time.dat"
        launch_file = f"{PARENT_DIR}/expe-blazegraph-baseline/launch.sh"

        # Writing the launch script for Blazegraph execution
        with open(launch_file, "w") as launch:
            launch.write(f"""#!/bin/bash
java -jar -Xmx52G /app/blazegraph-cli.jar --database=/blazegraph.jnl --file={docker_query_file} &> {docker_result}
if grep -q "Query timed out after" {docker_result}; then
    timeout_value=$(grep -oP 'Query timed out after \\K[0-9]+' {docker_result})
    echo 0 > {docker_nb_results}
    echo $timeout_value > {docker_time}
else
    nb_lines=$(($(wc -l < {docker_result}) - 2))
    if [ $nb_lines -lt 0 ]; then nb_lines=0; fi
    echo $nb_lines > {docker_nb_results}
    tail -n 1 {docker_result} | awk '{{print $2}}' >> {docker_time}
fi
""")

        # Ensuring the output directory exists
        os.makedirs(f"{PARENT_DIR}/expe-blazegraph-baseline/{wildcards.query_dir}/{wildcards.cpus}-cpus/{wildcards.run_id}", exist_ok=True)

        # Running the Docker container for Blazegraph execution
        shell(f"""
        docker load < {input.img} &&
        docker run --rm --cpuset-cpus="0-{cpus-1}" --memory="54G" \
        -v {input.data}:/blazegraph.jnl \
        -v {input.query}:{docker_query_file} \
        -v {PARENT_DIR}/expe-blazegraph-baseline/{wildcards.query_dir}/{wildcards.cpus}-cpus/{wildcards.run_id}:/{wildcards.query_dir}/{wildcards.cpus}-cpus/{wildcards.run_id} \
        -v {launch_file}:/app/launch.sh \
        --name bg-cli bg-cli /bin/bash -c "chmod a+x /app/launch.sh && /app/launch.sh"
        """)

rule join_data_for_query_over_blazegraph_embedded:
    input:
        nb_results=f"{PARENT_DIR}/expe-blazegraph-baseline/{{query_dir}}/{{cpus}}-cpus/{{run_id}}/{{query}}.nb.dat",
        time=f"{PARENT_DIR}/expe-blazegraph-baseline/{{query_dir}}/{{cpus}}-cpus/{{run_id}}/{{query}}.time.dat"
    output:
        out=f"{PARENT_DIR}/expe-blazegraph-baseline/{{query_dir}}/{{cpus}}-cpus/{{run_id}}/{{query}}.csv"
    run:
        with open(input.nb_results, "r") as nb_results_file:
            nb_results = int(nb_results_file.read().strip())

        with open(input.time, "r") as time_file:
            time_as_string = time_file.read().strip()

        with open(output.out, "w") as output_file:
            output_file.write("nb_results execution_time(ms)\n")
            output_file.write(f"{nb_results} {time_as_string}\n")

rule all_queries_and_collect_final_csv_embedded:
    input:
        lambda wildcards: expand(
                    f"{PARENT_DIR}/expe-blazegraph-baseline/{{query_dir}}/{{cpus}}-cpus/{{run_id}}/{{query}}.csv",
                    query_dir=wildcards.query_dir,
                    query=get_queries(wildcards.query_dir),
                    cpus=wildcards.cpus,
                    run_id=run_ids
        )
    output:
        final_csv=f"{PARENT_DIR}/expe-blazegraph-baseline/{{query_dir}}/report-{{query_dir}}-{{cpus}}-cpus.csv"
    run:
        print(f"Total number of CSV files: {len(input)}")
        print(f"Collecting final CSV for {input}")

        with open(output.final_csv, "w") as final_output:
            final_output.write("query_name run cpus nb_results execution_time(ms)\n")
            for csv_file in input:
                run_id = Path(csv_file).parent.name
                cpu = Path(csv_file).parent.parent.name
                if Path(csv_file).exists():
                    with open(csv_file, "r") as query_csv:
                        lines = query_csv.readlines()
                        if len(lines) > 1:
                            data_line = lines[1].strip()
                            query_name = Path(csv_file).stem
                            final_output.write(f"{query_name} {run_id} {cpu} {data_line}\n")

        print(f"Final CSV has been collected at {output.final_csv}")
